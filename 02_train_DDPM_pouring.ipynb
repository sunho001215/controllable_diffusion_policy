{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from loader.pouring_dataset import Pouring\n",
    "from models.ConditionalUNet1D import ConditionalUnet1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diffusion_iters = 100\n",
    "num_epochs = 2500\n",
    "batch_size = 10\n",
    "input_dim=12\n",
    "device = torch.device('cuda')\n",
    "save_path = \"./params/pouring_dataset/basic_DDPM/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pouring dataset is ready; # of trajectories: 10\n",
      "batch['traj'].shape: torch.Size([10, 480, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "dataset = Pouring()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['traj'].shape:\",batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=input_dim,\n",
    "    global_cond_dim=0\n",
    ")\n",
    "\n",
    "# DDPM scheduler\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # Squared cosine\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # Clip output to [-1,1]\n",
    "    clip_sample=True,\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "_ = noise_pred_net.to(device)\n",
    "\n",
    "# Exponential Moving Average\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75\n",
    ")\n",
    "\n",
    "# ADAM optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "# Consine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2500/2500 [04:27<00:00,  9.33it/s, loss=0.00666]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # SE3 to vec\n",
    "                ntraj = nbatch[0].to(device)\n",
    "                B, traj_len, _, _ = ntraj.shape\n",
    "                ntraj = ntraj[:, :, :3, :]\n",
    "                ntraj = ntraj.reshape(B, traj_len, -1)\n",
    "\n",
    "                # Sample noise\n",
    "                noise = torch.randn(ntraj.shape, device=device)\n",
    "\n",
    "                # Sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # Forward diffusion process\n",
    "                noisy_traj = noise_scheduler.add_noise(\n",
    "                    ntraj, noise, timesteps\n",
    "                )\n",
    "\n",
    "                # Predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_traj, timesteps\n",
    "                )\n",
    "                \n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # Optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # Update EMP\n",
    "                ema.step(noise_pred_net.parameters())\n",
    "\n",
    "                # Logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "ema_noise_pred_net = noise_pred_net\n",
    "ema.copy_to(ema_noise_pred_net.parameters())\n",
    "\n",
    "# Save EMA model\n",
    "torch.save(ema_noise_pred_net, save_path + \"model_ep\" + str(num_epochs) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
